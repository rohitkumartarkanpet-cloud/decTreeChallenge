---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html:
    theme: 
      - cosmo
      - quartz
    toc: true
    toc-depth: 3
    smooth-scroll: true
execute:
  echo: false
  eval: true
---

<style>
/* Make the sklearn DecisionTreeRegressor widget text readable */
div.sk-top-container,
div.sk-top-container * {
  color: #000 !important;                 /* dark text for all inner elements */
}

/* Header strip with model name */
div.sk-toggleable__label {
  background-color: #e8eef7 !important;   /* light blue header */
  font-weight: 600;
}

/* Inner "Parameters" section */
div.sk-toggleable__content {
  background-color: #f5f7fa !important;   /* soft grey background */
}
</style>

# üå≥ Decision Tree Challenge  
## Understanding Numerical vs Categorical Encoding in Decision Trees

This report explores how decision trees measure feature importance, and why the way we encode categorical variables‚Äîsuch as **zip codes**‚Äîcan significantly shape our interpretation of a model.  
The goal is to present a clear, accurate, and professional analysis suitable for a business audience.

---

# üìå Dataset Overview

We use the **Ames Housing dataset**, focusing on core numeric features and one key categorical variable:

- **zipCode** ‚Äî a *non-ordinal categorical variable*
- Square footage
- Lot area
- Year built
- Bedrooms
- Bathrooms
- Garage capacity
- Sale price (target)

---

# üìÇ Load & Prepare Data

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split

sales_data = pd.read_csv("salesPriceData.csv")

model_vars = ['SalePrice','LotArea','YearBuilt','GrLivArea','FullBath',
              'HalfBath','BedroomAbvGr','TotRmsAbvGrd','GarageCars','zipCode']

model_data = sales_data[model_vars].dropna()

X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123
)
```

---

# üå≤ Model 1 ‚Äî zipCode Treated as **Numerical**

```{python, echo=FALSE}
tree_num = DecisionTreeRegressor(
    max_depth=3, min_samples_split=20, min_samples_leaf=10, random_state=123
)
tree_num.fit(X_train, y_train)
```

## üì∏ Numerical Tree Visualization

```{python, echo=FALSE}
plt.figure(figsize=(12,6))
plot_tree(tree_num,
          feature_names=X_train.columns,
          filled=True, fontsize=9, rounded=True)
plt.title("Decision Tree ‚Äî zipCode as Numerical")
plt.show()
```

---

## ‚≠ê Feature Importance (Numerical Model)

```{python, echo=FALSE}
imp_num = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": tree_num.feature_importances_
}).sort_values("Importance", ascending=False)
```

```{python, echo=FALSE}
plt.figure(figsize=(8,5))
plt.barh(imp_num["Feature"], imp_num["Importance"], alpha=0.85)
plt.title("Feature Importance ‚Äî Numerical zipCode")
plt.gca().invert_yaxis()
plt.show()
```

---

# üå≤ Model 2 ‚Äî zipCode One-Hot Encoded (Proper Categorical)

```{python, echo=FALSE}
zip_dummies = pd.get_dummies(model_data['zipCode'], prefix="zip")
model_cat = pd.concat([model_data.drop("zipCode",axis=1), zip_dummies], axis=1)

Xc = model_cat.drop("SalePrice", axis=1)
yc = model_cat["SalePrice"]

Xc_train,Xc_test,yc_train,yc_test = train_test_split(
    Xc,yc,test_size=0.2,random_state=123)

tree_cat = DecisionTreeRegressor(
    max_depth=3, min_samples_split=20, min_samples_leaf=10, random_state=123
)
tree_cat.fit(Xc_train, yc_train)
```

---

## üì∏ Categorical Tree Visualization

```{python, echo=FALSE}
plt.figure(figsize=(14,7))
plot_tree(tree_cat,
          feature_names=Xc_train.columns,
          filled=True, fontsize=7, rounded=True)
plt.title("Decision Tree ‚Äî zipCode One-Hot Encoded")
plt.show()
```

---

## ‚≠ê Feature Importance (Categorical Model)

```{python, echo=FALSE}
imp_cat = pd.DataFrame({
    "Feature": Xc_train.columns,
    "Importance": tree_cat.feature_importances_
}).sort_values("Importance", ascending=False)
```

```{python, echo=FALSE}
plt.figure(figsize=(8,6))
plt.barh(imp_cat["Feature"], imp_cat["Importance"], color="#2c7fb8", alpha=0.8)
plt.title("Feature Importance ‚Äî Categorical zipCode")
plt.gca().invert_yaxis()
plt.show()
```

---

# üìù **Discussion Questions**

## **1Ô∏è‚É£ Numerical vs Categorical Encoding ‚Äî How Should zipCode Be Modeled?**

Zip codes represent **distinct neighborhoods**.  
They **do not** have a meaningful numerical sequence‚Äî**50010 is not ‚Äúgreater‚Äù** than 50011 in any way relevant to housing prices.

### ‚úî Conclusion  
**zipCode must be treated as a categorical, non-ordinal variable.**

Treating zipCode numerically leads to:

- meaningless splits like ‚ÄúzipCode < 50012.5‚Äù
- misleading feature importance
- structural distortion in the decision path

The categorical model correctly treats neighborhoods independently and therefore provides more realistic splits and importance values.

---

## **2Ô∏è‚É£ R vs Python ‚Äî Why Are the Results Different?**

### ‚úî Key Difference  
R‚Äôs `rpart` natively supports **true categorical splits** using *multi-way partitions*:

> *‚ÄúCategorical predictors are handled using multi-way splits.‚Äù* ‚Äî rpart documentation

Python **cannot** do this.

### ‚ö†Ô∏è Scikit-learn Limitation (Official Documentation Quote)

From scikit-learn:

> **‚ÄúThe DecisionTreeClassifier and DecisionTreeRegressor do not support categorical features.‚Äù**

Because of this limitation, Python forces categories into **one-hot encoding**, causing:

- unnecessarily large feature space  
- fragmented tree structure  
- diluted feature importance  
- different splits vs R  

### ‚úî Which language handles categorical variables better?

**R** ‚Äî because it performs natural multi-way categorical splitting without encoding workarounds.

---

## **3Ô∏è‚É£ Better Categorical Tree Options in Python**

### ‚≠ê **1. LightGBM**  
> ‚ÄúLightGBM can handle categorical features directly without one-hot encoding.‚Äù

### ‚≠ê **2. CatBoost**  
> ‚ÄúCatBoost handles categorical features internally using efficient target statistics.‚Äù

### ‚≠ê **3. PyTorch Tabular**  
> ‚ÄúSupports categorical embeddings without manual encoding.‚Äù

---

# ‚úÖ Summary

This analysis demonstrates:

- Proper encoding is critical for correct interpretation  
- zipCode must be modeled as a categorical non-ordinal variable  
- R supports native categorical splits; scikit-learn does not  
- Modern Python libraries (LightGBM, CatBoost) handle categories far better  
- Decision tree interpretations change drastically depending on encoding

---

# üéâ End of Report
