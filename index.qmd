---
title: "Decision Tree Challenge"
subtitle: "ZIP Encoding and Decision‑Tree Feature Importance (Ames Housing)"
format:
  html:
    theme:
      light: minty
      dark: slate
    toc: true
    toc-depth: 2
    toc-location: left
    code-fold: true
    code-summary: "Show code"
execute:
  echo: false
  warning: false
page-layout: full
smooth-scroll: true
---

# Overview

ZIP codes identify neighborhoods. Treating them as numbers forces arbitrary thresholds (e.g., `zipCode > 50012.5`) that have no geographic meaning. Treating them as categories lets the tree separate neighborhoods explicitly. We compare both choices on the *same* train/test split and then visualize differences with two new designs:

- **Butterfly comparison** (mirror bars): Numeric on the left vs Categorical on the right, per feature.
- **Rank bump**: how feature ranks change when ZIP is treated correctly.

No residual plots. Figures are sized to avoid horizontal scrolling.

---

# Data and Split

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import ticker, rcParams
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

rcParams['axes.titleweight'] = 'bold'
plt.rcParams['figure.dpi'] = 120

# Load analysis columns
df = pd.read_csv("salesPriceData.csv", usecols=[
    'SalePrice','LotArea','YearBuilt','GrLivArea','FullBath','HalfBath',
    'BedroomAbvGr','TotRmsAbvGrd','GarageCars','zipCode'
]).dropna().copy()

# Single split for a fair A/B comparison
X = df.drop('SalePrice', axis=1)
y = df['SalePrice']
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=123)
```

---

# Model A — ZIP encoded as numeric

```{python}
tree_num = DecisionTreeRegressor(max_depth=4, min_samples_split=20, min_samples_leaf=10, random_state=123)
tree_num.fit(X_tr, y_tr)

yhat_te_n = tree_num.predict(X_te)

# Metrics (version-agnostic)
r2_te_n = 1 - ((y_te - yhat_te_n)**2).sum() / ((y_te - y_te.mean())**2).sum()
rmse_te_n = ((y_te - yhat_te_n)**2).mean()**0.5

imp_num = (pd.DataFrame({'Feature': X_tr.columns, 'Importance': tree_num.feature_importances_})
           .sort_values('Importance', ascending=False)
           .reset_index(drop=True))
```

### Tree snapshot

```{python}
#| fig-width: 9
#| fig-height: 5
fig = plt.figure(figsize=(9,5))
plot_tree(tree_num, feature_names=X_tr.columns, filled=True, rounded=True, fontsize=9)
plt.title("Decision Tree — ZIP as Numeric", fontsize=14)
plt.tight_layout(); plt.show()
```

---

# Model B — ZIP encoded as categorical (one‑hot)

```{python}
# One‑hot for ZIP
zip_dum = pd.get_dummies(df['zipCode'], prefix='zip')
dfc = pd.concat([df.drop('zipCode', axis=1), zip_dum], axis=1)

Xc = dfc.drop('SalePrice', axis=1)
yc = dfc['SalePrice']
Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(Xc, yc, test_size=0.20, random_state=123)

tree_cat = DecisionTreeRegressor(max_depth=4, min_samples_split=20, min_samples_leaf=10, random_state=123)
tree_cat.fit(Xc_tr, yc_tr)

yhat_te_c = tree_cat.predict(Xc_te)

r2_te_c = 1 - ((yc_te - yhat_te_c)**2).sum() / ((yc_te - yc_te.mean())**2).sum()
rmse_te_c = ((yc_te - yhat_te_c)**2).mean()**0.5

imp_cat = (pd.DataFrame({'Feature': Xc_tr.columns, 'Importance': tree_cat.feature_importances_})
           .sort_values('Importance', ascending=False)
           .reset_index(drop=True))

zip_total = float(imp_cat.loc[imp_cat['Feature'].str.startswith('zip_'), 'Importance'].sum())
```

### Tree snapshot

```{python}
#| fig-width: 9
#| fig-height: 5
fig = plt.figure(figsize=(9,5))
plot_tree(tree_cat, feature_names=list(Xc_tr.columns), filled=True, rounded=True, fontsize=8, max_depth=4)
plt.title("Decision Tree — ZIP as Categorical (One‑Hot)", fontsize=14)
plt.tight_layout(); plt.show()
```

---

# Butterfly Comparison (Numeric vs Categorical)

We compare **shared base features** and add one row for **ZIP (aggregated)** on the categorical side.

```{python}
# Prepare aligned table
base_feats = ['LotArea','YearBuilt','GrLivArea','FullBath','HalfBath','BedroomAbvGr','TotRmsAbvGrd','GarageCars']

def get_imp(frame, name):
    return float(frame.loc[frame['Feature']==name, 'Importance'].sum())

comp = pd.DataFrame({
    'Feature': base_feats + ['ZIP'],
    'Numeric': [get_imp(imp_num, f) for f in base_feats] + [
        get_imp(imp_num, 'zipCode') if 'zipCode' in imp_num['Feature'].values else 0.0
    ],
    'Categorical': [get_imp(imp_cat, f) for f in base_feats] + [zip_total]
})

# Sort by max share across models for a stable order
comp['maxshare'] = comp[['Numeric','Categorical']].max(axis=1)
comp = comp.sort_values('maxshare', ascending=True).drop(columns='maxshare').reset_index(drop=True)
```

```{python}
#| fig-width: 10
#| fig-height: 6
fig = plt.figure(figsize=(10,6))
gs = fig.add_gridspec(1,2, width_ratios=[1,1], wspace=0.04)
axL = fig.add_subplot(gs[0,0])
axR = fig.add_subplot(gs[0,1], sharey=axL)

y = np.arange(len(comp))

# Left: Numeric (mirror to the left as negative)
axL.barh(y, -comp['Numeric'], color='#3d8bfd', alpha=0.9, label='Numeric')
axL.set_xlim(-max(0.01, comp['Numeric'].max()*1.15), 0)
axL.set_xlabel('Numeric share')
axL.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=0))
for yi, val in zip(y, comp['Numeric']):
    if val>0:
        axL.text(-val - 0.01*max(0.01, comp['Numeric'].max()), yi, f"{val*100:.1f}%", va='center', ha='right', fontsize=9)

# Right: Categorical
axR.barh(y, comp['Categorical'], color='#00b894', alpha=0.9, label='Categorical')
axR.set_xlim(0, max(0.01, comp['Categorical'].max())*1.15)
axR.set_xlabel('Categorical share')
axR.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=0))
for yi, val in zip(y, comp['Categorical']):
    if val>0:
        axR.text(val + 0.01*max(0.01, comp['Categorical'].max()), yi, f"{val*100:.1f}%", va='center', ha='left', fontsize=9)

# Shared y‑labels
axL.set_yticks(y, comp['Feature'])
axR.set_yticks(y, [])
axL.set_title("Left: ZIP as Numeric", fontsize=12)
axR.set_title("Right: ZIP as Categorical", fontsize=12)
plt.suptitle("Butterfly Comparison — Feature Importance (share)", fontsize=14, y=0.98)
plt.tight_layout(rect=[0,0,1,0.95]); plt.show()
```

---

# Rank Bump (how ordering changes)

```{python}
# Build rank tables
rank_num = comp[['Feature','Numeric']].copy()
rank_cat = comp[['Feature','Categorical']].copy()
rank_num['Rank_Numeric'] = rank_num['Numeric'].rank(ascending=False, method='min')
rank_cat['Rank_Categorical'] = rank_cat['Categorical'].rank(ascending=False, method='min')
ranks = rank_num[['Feature','Rank_Numeric']].merge(rank_cat[['Feature','Rank_Categorical']], on='Feature')

# Order features by average rank for a tidy layout
ranks['avg'] = (ranks['Rank_Numeric'] + ranks['Rank_Categorical'])/2
ranks = ranks.sort_values('avg', ascending=False)  # top at top
```

```{python}
#| fig-width: 10
#| fig-height: 6
fig, ax = plt.subplots(figsize=(10,6))
xpos = [0, 1]
for i, row in ranks.iterrows():
    ax.plot(xpos, [row['Rank_Numeric'], row['Rank_Categorical']], marker='o', linewidth=2)
    ax.text(-0.05, row['Rank_Numeric'], row['Feature'], va='center', ha='right', fontsize=9)
ax.set_xticks(xpos, ['Numeric ZIP', 'Categorical ZIP'])
ax.invert_yaxis()
ax.set_ylabel("Rank (1 = most important)")
ax.set_title("Rank Bump — Feature Order Shift")
plt.tight_layout(); plt.show()
```

---

# Side‑by‑side Metrics

```{python}
summary = pd.DataFrame({
    'Model': ['ZIP as Numeric','ZIP as Categorical'],
    'Test R²': [r2_te_n, r2_te_c],
    'Test RMSE': [rmse_te_n, rmse_te_c],
    'ZIP Total Importance (share)': [
        float(imp_num.loc[imp_num['Feature']=='zipCode','Importance'].sum()) if 'zipCode' in imp_num['Feature'].values else 0.0,
        float(zip_total)
    ]
})
summary
```

```{python}
def style_table(df):
    return (df.style.format({'Test R²': '{:.3f}','Test RMSE': '{:,.0f}','ZIP Total Importance (share)': '{:.2%}'})
            .set_table_styles([
                {'selector':'th','props':'background-color:#0b2239;color:#fff;font-weight:bold;text-align:center;'},
                {'selector':'td','props':'text-align:center; padding:6px; font-size:13px;'}
            ])
            .background_gradient(cmap='Blues', subset=['Test R²'])
            .background_gradient(cmap='Oranges', subset=['Test RMSE'])
            .background_gradient(cmap='Greens', subset=['ZIP Total Importance (share)'])
            .hide(axis='index'))
style_table(summary)
```

---

# Interpretation

- Numeric ZIP suppresses neighborhood information by imposing an arbitrary order.  
- Categorical ZIP surfaces neighborhood effects; the aggregated ZIP share reflects its contribution.  
- For location‑driven problems, encode ZIP as categorical, or use tree methods with native categorical support.

---

# Discussion (required answers)

## 1) Numerical vs categorical encoding for ZIP

**Recommendation:** Model ZIP **as a categorical, non‑ordinal** variable. ZIP codes label neighborhoods; there is no meaningful “greater than / less than” relation among codes. Encoding ZIP as numbers forces artificial thresholds (e.g., `> 50012.5`). Encoding as categories (via one‑hot or, better, a model with native categorical support) lets the tree separate neighborhoods directly. In our results, the **aggregated ZIP importance** appears only when encoded as categorical, which aligns with economic intuition that location drives price.

**Bottom line:** ZIP → **categorical (non‑ordinal).**

## 2) R vs Python when ZIP is categorical

- **R (`rpart`)** treats string/factor columns as **factors** natively. Trees can split on subsets of factor levels without manual encoding, so neighborhood effects appear directly in the splits.  
- **Python (`sklearn.tree.DecisionTreeRegressor`)** requires **numeric input**. Categorical features must be encoded (one‑hot or ordinal). This encoding choice changes the split search space and how importance credit is distributed across dummies.

**Documentation‑based evidence (scikit‑learn):**

> “Able to handle both numerical and categorical data. **However, the scikit‑learn implementation does not support categorical variables for now.**” — *Decision Trees*, scikit‑learn documentation.

**Implication:** With one‑hot encoding, ZIP contributes through many dummy columns; credit is smeared and tree depth constraints may block neighborhood splits that a factor‑aware tree in R would find.

## 3) Better categorical handling in Python (suggestions)

If categories matter (like ZIP), prefer libraries with **native categorical support**:

- **CatBoost** (Yandex): “**Do not use one‑hot encoding during preprocessing.** … CatBoost supports numerical, categorical, text, and embeddings features.”  
- **LightGBM** (Microsoft): “LightGBM offers good accuracy with **integer‑encoded categorical features**… Use `categorical_feature` to specify the categorical features.”  

**Practical guidance for this project:** keep the scikit‑learn baseline (for teaching) but flag that production models with location categories should consider **CatBoost** or **LightGBM** to avoid distortions introduced by manual one‑hot encoding.

---

# Conclusions

- Treat ZIP as **categorical (non‑ordinal)**.  
- In scikit‑learn, categories must be encoded; this affects splits and importance. R’s `rpart` handles factors natively.  
- For serious models with strong categorical signals, prefer **CatBoost** or **LightGBM** (native categorical handling).
